{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Deep Learning\n",
    "\n",
    "8.1 Use a pre-trained ResNet50 and VGG16 on Keras to implement an inference of the test image in this directory!\n",
    "\n",
    "* Example: <https://keras.io/applications/>\n",
    "\n",
    "\n",
    "8.2 Use a pre-trained ResNet50 and VGG16 on PyTorch to implement an inference of the test image in this directory!\n",
    "\n",
    "* Example: http://pytorch.org/docs/master/torchvision/models.html\n",
    "\n",
    "8.3 Compare the inference times of both networks and frameworks!\n",
    "\n",
    "8.4 Find two positive and negative sample images that are correctly/incorrectly classified!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n04604644', 'worm_fence', 0.07046531), ('n02793495', 'barn', 0.06862043), ('n03000134', 'chainlink_fence', 0.061941862)]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions as decode_predictions_res\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = 'test_image.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "k_res_start = time.time()\n",
    "preds = model.predict(x)\n",
    "k_res_time = time.time()-k_res_start\n",
    "preds\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions_res(preds, top=3)[0])\n",
    "# Predicted: [(u'n02504013', u'Indian_elephant', 0.82658225), (u'n01871265', u'tusker', 0.1122357), (u'n02504458', u'African_elephant', 0.061040461)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [('n02793495', 'barn', 0.07425918), ('n04604644', 'worm_fence', 0.058995645), ('n04326547', 'stone_wall', 0.039503533), ('n03891251', 'park_bench', 0.027926859), ('n02965783', 'car_mirror', 0.025498722)]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions as decode_predictions_vgg\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "img_path = 'test_image.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "k_vgg_start = time.time()\n",
    "features = model.predict(x)\n",
    "k_vgg_time = time.time()-k_vgg_start\n",
    "\n",
    "print('Predicted:', decode_predictions_vgg(features)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n04604644', 'worm_fence', 8.481195),\n",
       " ('n02793495', 'barn', 8.475863),\n",
       " ('n03891251', 'park_bench', 8.366762)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.utils\n",
    "import torchvision.transforms as transforms\n",
    "import torch.autograd.variable as Variable\n",
    "from PIL import Image\n",
    "\n",
    "resnet50 = models.resnet50(pretrained=True).eval()\n",
    "#resnet50 = models.resnet50(pretrained=True)\n",
    "\n",
    "path_img = 'test_image.jpg'\n",
    "image = Image.open(path_img)\n",
    "transformation = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "image_tensor = transformation(image).float()\n",
    "image_tensor = image_tensor.unsqueeze_(0)\n",
    "input = Variable(image_tensor)\n",
    "\n",
    "t_res_start = time.time()\n",
    "output = resnet50(input)\n",
    "t_res_time = time.time()-t_res_start\n",
    "decode_predictions_res(output.detach().numpy(), top=3)[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('n02793495', 'barn', 9.087323),\n",
       " ('n04604644', 'worm_fence', 8.780945),\n",
       " ('n03891251', 'park_bench', 8.334769),\n",
       " ('n04532670', 'viaduct', 7.6061187),\n",
       " ('n09332890', 'lakeside', 7.384339)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.utils\n",
    "import torchvision.transforms as transforms\n",
    "import torch.autograd.variable as Variable\n",
    "from PIL import Image\n",
    "\n",
    "vgg16 = models.vgg16(pretrained=True).eval()\n",
    "\n",
    "t_vgg_start = time.time()\n",
    "output = vgg16(input)\n",
    "t_vgg_time = time.time()-t_vgg_start\n",
    "decode_predictions_vgg(output.detach().numpy())[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.112948179244995, 1.7309412956237793, 0.6141815185546875, 1.809493064880371]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k_res_time,k_vgg_time,t_res_time,t_vgg_time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('n02484975', 'guenon', 13.237154), ('n02488702', 'colobus', 12.235919), ('n01622779', 'great_grey_owl', 10.033047)]\n",
      "[('n01968897', 'chambered_nautilus', 10.218805), ('n01910747', 'jellyfish', 9.710601), ('n03775546', 'mixing_bowl', 9.596883)]\n",
      "[('n09472597', 'volcano', 12.595714), ('n03388043', 'fountain', 11.021697), ('n03729826', 'matchstick', 10.211475)]\n",
      "[('n02948072', 'candle', 10.652655), ('n03729826', 'matchstick', 10.208814), ('n03590841', \"jack-o'-lantern\", 9.003959)]\n"
     ]
    }
   ],
   "source": [
    "for i in [\"img1\",\"img2\",\"img3\",\"img4\"]:\n",
    "    path_img = i + \".jpg\"\n",
    "    image = Image.open(path_img)\n",
    "    transformation = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "    image_tensor = transformation(image).float()\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "\n",
    "    t_res_start = time.time()\n",
    "    output = resnet50(input)\n",
    "    t_res_time = time.time()-t_res_start\n",
    "    print(decode_predictions_res(output.detach().numpy(), top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1: true\n",
    "img2: false \n",
    "img3: false\n",
    "img4: false"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
