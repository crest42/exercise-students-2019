{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. MapReduce\n",
    "\n",
    "For this exercise we are going to use MapReduce in local mode, i.e. we won't be running anything on the cluster!\n",
    " \n",
    "## 3.1. Use the commands `head`, `cat`, `uniq`, `wc`, `sort`, `find`, `xargs`, `awk` to evaluate the NASA log file:\n",
    "\n",
    "* Data File:  <https://github.com/scalable-infrastructure/exercise-2018/blob/master/data/nasa/NASA_access_log_Jul95.gz>\n",
    "* Which page was called the most?\n",
    "* What was the most frequent return code?\n",
    "* How many errors occurred? What is the percentage of errors?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/naslx/projects/pn69si/mnmda001/students/mnmda050/exercise-students-2019/03_MapReduce\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!gzip -k -d -f  ../data/nasa/NASA_access_log_Jul95.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245\n",
      "unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085\n",
      "burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0\n",
      "199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0\n",
      "burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0\n",
      "205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985\n",
      "d104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985\n",
      "129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074\n"
     ]
    }
   ],
   "source": [
    "!head ../data/nasa/NASA_access_log_Jul95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Implement a Python version of this Unix Shell script using this script as template! Run the Python script inside an Hadoop Streaming job.\n",
    "\n",
    "Template: <https://github.com/scalable-infrastructure/scalable-infrastructure.github.io/blob/master/src/map_reduce.py>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "TMP_DIR=os.path.join(os.getcwd(), \"tmp\")\n",
    "os.environ[\"HADOOP_HOME\"]=\"/naslx/projects/pn69si/mnmda001/students/software/hadoop-2.8.5\"\n",
    "os.environ[\"JAVA_HOME\"]=\"/lrz/sys/compilers/java/jdk1.8.0_112\"\n",
    "os.environ[\"JAVA_OPTS\"]=\"-Djava.io.tmpdir=\"+TMP_DIR\n",
    "os.environ[\"HADOOP_OPTS\"]=\"-Djava.io.tmpdir=\"+TMP_DIR\n",
    "os.environ[\"PATH\"]=os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\") + \":\"+os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: Insufficient space for shared memory file:\n",
      "   28150\n",
      "Try using the -Djava.io.tmpdir= option to select an alternate temp location.\n",
      "\n",
      "Usage: $HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar [options]\n",
      "Options:\n",
      "  -input          <path> DFS input file(s) for the Map step.\n",
      "  -output         <path> DFS output directory for the Reduce step.\n",
      "  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n",
      "  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n",
      "  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n",
      "  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n",
      "                  Deprecated. Use generic option \"-files\" instead.\n",
      "  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n",
      "                  Optional. The input format class.\n",
      "  -outputformat   <TextOutputFormat(default)|JavaClassName>\n",
      "                  Optional. The output format class.\n",
      "  -partitioner    <JavaClassName>  Optional. The partitioner class.\n",
      "  -numReduceTasks <num> Optional. Number of reduce tasks.\n",
      "  -inputreader    <spec> Optional. Input recordreader spec.\n",
      "  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n",
      "  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n",
      "  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n",
      "  -io             <identifier> Optional. Format to use for input to and output\n",
      "                  from mapper/reducer commands\n",
      "  -lazyOutput     Optional. Lazily create Output.\n",
      "  -background     Optional. Submit the job and don't wait till it completes.\n",
      "  -verbose        Optional. Print verbose output.\n",
      "  -info           Optional. Print detailed usage.\n",
      "  -help           Optional. Print help message.\n",
      "\n",
      "Generic options supported are\n",
      "-conf <configuration file>     specify an application configuration file\n",
      "-D <property=value>            use value for given property\n",
      "-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n",
      "-jt <local|resourcemanager:port>    specify a ResourceManager\n",
      "-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n",
      "-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n",
      "-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n",
      "\n",
      "The general command line syntax is\n",
      "command [genericOptions] [commandOptions]\n",
      "\n",
      "\n",
      "Usage tips:\n",
      "In -input: globbing on <path> is supported and can have multiple -input\n",
      "\n",
      "Default Map input format: a line is a record in UTF-8 the key part ends at first\n",
      "  TAB, the rest of the line is the value\n",
      "\n",
      "To pass a Custom input format:\n",
      "  -inputformat package.MyInputFormat\n",
      "\n",
      "Similarly, to pass a custom output format:\n",
      "  -outputformat package.MyOutputFormat\n",
      "\n",
      "The files with extensions .class and .jar/.zip, specified for the -file\n",
      "  argument[s], end up in \"classes\" and \"lib\" directories respectively inside\n",
      "  the working directory when the mapper and reducer are run. All other files\n",
      "  specified for the -file argument[s] end up in the working directory when the\n",
      "  mapper and reducer are run. The location of this working directory is\n",
      "  unspecified.\n",
      "\n",
      "To set the number of reduce tasks (num. of output files) as, say 10:\n",
      "  Use -numReduceTasks 10\n",
      "To skip the sort/combine/shuffle/sort/reduce step:\n",
      "  Use -numReduceTasks 0\n",
      "  Map output then becomes a 'side-effect output' rather than a reduce input.\n",
      "  This speeds up processing. This also feels more like \"in-place\" processing\n",
      "  because the input filename and the map input order are preserved.\n",
      "  This is equivalent to -reducer NONE\n",
      "\n",
      "To speed up the last maps:\n",
      "  -D mapreduce.map.speculative=true\n",
      "To speed up the last reduces:\n",
      "  -D mapreduce.reduce.speculative=true\n",
      "To name the job (appears in the JobTracker Web UI):\n",
      "  -D mapreduce.job.name='My Job'\n",
      "To change the local temp directory:\n",
      "  -D dfs.data.dir=/tmp/dfs\n",
      "  -D stream.tmpdir=/tmp/streaming\n",
      "Additional local temp directories with -jt local:\n",
      "  -D mapreduce.cluster.local.dir=/tmp/local\n",
      "  -D mapreduce.jobtracker.system.dir=/tmp/system\n",
      "  -D mapreduce.cluster.temp.dir=/tmp/temp\n",
      "To treat tasks with non-zero exit status as SUCCEDED:\n",
      "  -D stream.non.zero.exit.is.failure=false\n",
      "Use a custom hadoop streaming build along with standard hadoop install:\n",
      "  $HADOOP_PREFIX/bin/hadoop jar /path/my-hadoop-streaming.jar [...]\\\n",
      "    [...] -D stream.shipped.hadoopstreaming=/path/my-hadoop-streaming.jar\n",
      "For more details about jobconf parameters see:\n",
      "  http://wiki.apache.org/hadoop/JobConfFile\n",
      "Truncate the values of the job configuration copiedto the environment at the given length:\n",
      "   -D stream.jobconf.truncate.limit=-1\n",
      "To set an environment variable in a streaming command:\n",
      "   -cmdenv EXAMPLE_DIR=/home/example/dictionaries/\n",
      "\n",
      "Shortcut:\n",
      "   setenv HSTREAMING \"$HADOOP_PREFIX/bin/hadoop jar hadoop-streaming.jar\"\n",
      "\n",
      "Example: $HSTREAMING -mapper \"/usr/local/bin/perl5 filter.pl\"\n",
      "           -file /local/filter.pl -input \"/logs/0604*/*\" [...]\n",
      "  Ships a script, invokes the non-shipped perl interpreter. Shipped files go to\n",
      "  the working directory so filter.pl is found by perl. Input files are all the\n",
      "  daily logs for days in month 2006-04\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar ${HADOOP_HOME}/share/hadoop/tools/lib/hadoop-streaming-2.8.5.jar -info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head ../data/nasa/NASA_access_log_Jul95| python map_reduce.py map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3.3 Run the program Terasort on 1 GB of data - each record that TeraGen generates is 100 Bytes in size:\n",
    "\n",
    "    hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar teragen <number_of_records> <output_directory>\n",
    "\n",
    "    hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar terasort <input_directory> <output_directory>\n",
    "    \n",
    "    hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.5.jar teravalidate <input_directory> <output_directory>\n",
    "\n",
    "Measure the runtime for each step and plot the results! ***Use the `/tmp` directory as output directory!***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: Insufficient space for shared memory file:\n",
      "   28580\n",
      "Try using the -Djava.io.tmpdir= option to select an alternate temp location.\n",
      "\n",
      "teragen <num rows> <output dir>\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar teragen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf teraout-100MB\n",
    "!hadoop jar ${HADOOP_HOME}/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar terasort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
